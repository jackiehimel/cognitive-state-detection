"""
Module for training and evaluating cognitive state detection models.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import pickle
import os
import logging
import joblib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CognitiveStateDetector:
    """
    Class for training and using cognitive state detection models.
    """
    
    def __init__(self, models_dir='models'):
        """
        Initialize the cognitive state detector.
        
        Parameters:
        -----------
        models_dir : str
            Directory to save trained models
        """
        self.models_dir = models_dir
        self.fatigue_model = None
        self.frustration_model = None
        self.scaler = StandardScaler()
        
        # Create models directory if it doesn't exist
        os.makedirs(models_dir, exist_ok=True)
        
        logger.info("CognitiveStateDetector initialized")
    
    def _prepare_feature_vectors(self, features):
        """
        Prepare feature vectors from extracted features for model training.
        
        Parameters:
        -----------
        features : dict
            Dictionary containing extracted features
            
        Returns:
        --------
        tuple
            (X, y_fatigue, y_frustration) - feature matrix and labels
        """
        logger.info("Preparing feature vectors for model training")
        
        # Initialize lists for feature vectors and labels
        X = []
        y_fatigue = []
        y_frustration = []
        
        # Process all feature entries
        for i in range(len(features['labels'])):
            label = features['labels'][i]
            
            # Skip entries with missing label
            if not label:
                continue
                
            # Combine all feature types into a single vector
            feature_vector = []
            
            # Add blink pattern features if available
            if i < len(features['blink_patterns']) and features['blink_patterns'][i]:
                blink_pattern = features['blink_patterns'][i]
                for key in sorted(blink_pattern.keys()):
                    if isinstance(blink_pattern[key], (int, float)):
                        feature_vector.append(blink_pattern[key])
            
            # Add eye closure pattern features if available
            if i < len(features['eye_closure_patterns']) and features['eye_closure_patterns'][i]:
                eye_closure = features['eye_closure_patterns'][i]
                for key in sorted(eye_closure.keys()):
                    if isinstance(eye_closure[key], (int, float)):
                        feature_vector.append(eye_closure[key])
            
            # Add pupil metric features if available
            if i < len(features['pupil_metrics']) and features['pupil_metrics'][i]:
                pupil_metrics = features['pupil_metrics'][i]
                for key in sorted(pupil_metrics.keys()):
                    if isinstance(pupil_metrics[key], (int, float)):
                        feature_vector.append(pupil_metrics[key])
            
            # Add gaze pattern features if available
            if i < len(features['gaze_patterns']) and features['gaze_patterns'][i]:
                gaze_patterns = features['gaze_patterns'][i]
                for key in sorted(gaze_patterns.keys()):
                    if isinstance(gaze_patterns[key], (int, float)):
                        feature_vector.append(gaze_patterns[key])
            
            # Skip entries with empty feature vectors
            if not feature_vector:
                continue
                
            # Add feature vector to matrix
            X.append(feature_vector)
            
            # Create binary labels for fatigue and frustration
            y_fatigue.append(1 if label == 'fatigue' else 0)
            y_frustration.append(1 if label == 'frustration' else 0)
        
        logger.info(f"Prepared {len(X)} feature vectors for model training")
        
        # Convert to numpy arrays
        X = np.array(X)
        y_fatigue = np.array(y_fatigue)
        y_frustration = np.array(y_frustration)
        
        return X, y_fatigue, y_frustration
    
    def train_models(self, features, cv=5, grid_search=True):
        """
        Train fatigue and frustration detection models.
        
        Parameters:
        -----------
        features : dict
            Dictionary containing extracted features
        cv : int
            Number of cross-validation folds
        grid_search : bool
            Whether to perform grid search for hyperparameter tuning
            
        Returns:
        --------
        dict
            Dictionary containing trained models and cross-validation scores
        """
        logger.info("Training cognitive state detection models")
        
        # Prepare feature vectors and labels
        X, y_fatigue, y_frustration = self._prepare_feature_vectors(features)
        
        # Standardize features
        X_scaled = self.scaler.fit_transform(X)
        
        # Train fatigue detection model
        logger.info("Training fatigue detection model")
        if grid_search:
            # Define parameter grid for Random Forest
            param_grid_rf = {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
            
            # Grid search with cross-validation
            grid_search_rf = GridSearchCV(
                RandomForestClassifier(random_state=42),
                param_grid_rf,
                cv=cv,
                scoring='f1',
                verbose=1,
                n_jobs=-1
            )
            
            # Train model
            grid_search_rf.fit(X_scaled, y_fatigue)
            
            # Get best model
            self.fatigue_model = grid_search_rf.best_estimator_
            logger.info(f"Best fatigue model parameters: {grid_search_rf.best_params_}")
        else:
            # Train with default parameters
            self.fatigue_model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.fatigue_model.fit(X_scaled, y_fatigue)
        
        # Cross-validation scores for fatigue model
        fatigue_cv_scores = cross_val_score(self.fatigue_model, X_scaled, y_fatigue, cv=cv, scoring='f1')
        logger.info(f"Fatigue model cross-validation F1 scores: {fatigue_cv_scores}")
        logger.info(f"Mean F1 score: {np.mean(fatigue_cv_scores)}")
        
        # Train frustration detection model
        logger.info("Training frustration detection model")
        if grid_search:
            # Define parameter grid for SVM
            param_grid_svm = {
                'C': [0.1, 1.0, 10.0],
                'kernel': ['linear', 'rbf', 'poly'],
                'gamma': ['scale', 'auto', 0.1, 0.01]
            }
            
            # Grid search with cross-validation
            grid_search_svm = GridSearchCV(
                SVC(probability=True, random_state=42),
                param_grid_svm,
                cv=cv,
                scoring='f1',
                verbose=1,
                n_jobs=-1
            )
            
            # Train model
            grid_search_svm.fit(X_scaled, y_frustration)
            
            # Get best model
            self.frustration_model = grid_search_svm.best_estimator_
            logger.info(f"Best frustration model parameters: {grid_search_svm.best_params_}")
        else:
            # Train with default parameters
            self.frustration_model = SVC(probability=True, kernel='rbf', C=1.0, random_state=42)
            self.frustration_model.fit(X_scaled, y_frustration)
        
        # Cross-validation scores for frustration model
        frustration_cv_scores = cross_val_score(self.frustration_model, X_scaled, y_frustration, cv=cv, scoring='f1')
        logger.info(f"Frustration model cross-validation F1 scores: {frustration_cv_scores}")
        logger.info(f"Mean F1 score: {np.mean(frustration_cv_scores)}")
        
        # Save models
        logger.info("Saving trained models")
        joblib.dump(self.scaler, os.path.join(self.models_dir, 'scaler.pkl'))
        joblib.dump(self.fatigue_model, os.path.join(self.models_dir, 'fatigue_model.pkl'))
        joblib.dump(self.frustration_model, os.path.join(self.models_dir, 'frustration_model.pkl'))
        
        # Return results
        return {
            'fatigue_model': self.fatigue_model,
            'frustration_model': self.frustration_model,
            'fatigue_cv_scores': fatigue_cv_scores,
            'frustration_cv_scores': frustration_cv_scores
        }
    
    def load_models(self):
        """
        Load pre-trained models from disk.
        
        Returns:
        --------
        bool
            True if models were loaded successfully, False otherwise
        """
        logger.info("Loading pre-trained models")
        
        # Check if model files exist
        scaler_path = os.path.join(self.models_dir, 'scaler.pkl')
        fatigue_model_path = os.path.join(self.models_dir, 'fatigue_model.pkl')
        frustration_model_path = os.path.join(self.models_dir, 'frustration_model.pkl')
        
        if not all(os.path.exists(p) for p in [scaler_path, fatigue_model_path, frustration_model_path]):
            logger.warning("One or more model files not found")
            return False
            
        try:
            # Load models
            self.scaler = joblib.load(scaler_path)
            self.fatigue_model = joblib.load(fatigue_model_path)
            self.frustration_model = joblib.load(frustration_model_path)
            
            logger.info("Models loaded successfully")
            return True
        except Exception as e:
            logger.error(f"Error loading models: {str(e)}")
            return False
    def predict(self, features, fatigue_threshold=0.5, frustration_threshold=0.5, prefer_higher_confidence=True):
        """
        Predict cognitive state from extracted features.
        
        Parameters:
        -----------
        features : dict
            Dictionary containing extracted features
        fatigue_threshold : float, optional
            Threshold for fatigue detection (default: 0.5)
        frustration_threshold : float, optional
            Threshold for frustration detection (default: 0.5)
        prefer_higher_confidence : bool, optional
            If True, choose the state with higher confidence when multiple thresholds are exceeded
            
        Returns:
        --------
        dict
            Dictionary containing cognitive state and probabilities
        """
        if self.fatigue_model is None or self.frustration_model is None:
            logger.error("Models not loaded. Call train_models() or load_models() first.")
            return None
            
        # Extract features from dictionary
        feature_vector = []
        for key in sorted(features.keys()):
            if isinstance(features[key], (int, float)):
                feature_vector.append(features[key])
        
        # Reshape to 2D array (required for scikit-learn)
        X = np.array(feature_vector).reshape(1, -1)
        
        # Standardize features
        X_scaled = self.scaler.transform(X)
        
        # Make predictions
        fatigue_prob = self.fatigue_model.predict_proba(X_scaled)[0, 1]
        frustration_prob = self.frustration_model.predict_proba(X_scaled)[0, 1]
        
        # Calculate neutral probability (1 minus the others, ensuring it's not negative)
        neutral_prob = max(0, 1.0 - (fatigue_prob + frustration_prob))
        
        # Determine cognitive state based on thresholds and confidence
        cognitive_state = 'neutral'
        fatigue_detected = fatigue_prob >= fatigue_threshold
        frustration_detected = frustration_prob >= frustration_threshold
        
        if fatigue_detected and frustration_detected:
            # Both states detected, choose based on higher confidence
            if prefer_higher_confidence:
                cognitive_state = 'fatigue' if fatigue_prob > frustration_prob else 'frustration'
            else:
                # Default to frustration as it might require more immediate attention
                cognitive_state = 'frustration'
        elif fatigue_detected:
            cognitive_state = 'fatigue'
        elif frustration_detected:
            cognitive_state = 'frustration'
        
        # Return complete prediction results
        return {
            'cognitive_state': cognitive_state,
            'fatigue_probability': fatigue_prob,
            'frustration_probability': frustration_prob,
            'neutral_probability': neutral_prob,
            'confidence': max(fatigue_prob, frustration_prob, neutral_prob),
            'thresholds': {
                'fatigue': fatigue_threshold,
                'frustration': frustration_threshold
            }
        }
    logger.info("Saving trained models")
    joblib.dump(self.scaler, os.path.join(self.models_dir, 'scaler.pkl'))
    joblib.dump(self.fatigue_model, os.path.join(self.models_dir, 'fatigue_model.pkl'))
    joblib.dump(self.frustration_model, os.path.join(self.models_dir, 'frustration_model.pkl'))
        
    # Return results
    return {
        'fatigue_model': self.fatigue_model,
        'frustration_model': self.frustration_model,
        'fatigue_cv_scores': fatigue_cv_scores,
        'frustration_cv_scores': frustration_cv_scores
    }
    
def load_models(self):
    """
    Load pre-trained models from disk.
        
    Returns:
    --------
    bool
        True if models were loaded successfully, False otherwise
    """
    logger.info("Loading pre-trained models")
        
    # Check if model files exist
    scaler_path = os.path.join(self.models_dir, 'scaler.pkl')
    fatigue_model_path = os.path.join(self.models_dir, 'fatigue_model.pkl')
    frustration_model_path = os.path.join(self.models_dir, 'frustration_model.pkl')
        
    if not all(os.path.exists(p) for p in [scaler_path, fatigue_model_path, frustration_model_path]):
        logger.warning("One or more model files not found")
        return False
            
    try:
        # Load models
        self.scaler = joblib.load(scaler_path)
        self.fatigue_model = joblib.load(fatigue_model_path)
        self.frustration_model = joblib.load(frustration_model_path)
            
        logger.info("Models loaded successfully")
        return True
    except Exception as e:
        logger.error(f"Error loading models: {str(e)}")
        return False
    
def predict(self, features, fatigue_threshold=0.5, frustration_threshold=0.5, prefer_higher_confidence=True):
    """
    Predict cognitive state from extracted features.
        
    Parameters:
    -----------
            
Returns:
--------
dict
    Dictionary containing cognitive state and probabilities
"""
if self.fatigue_model is None or self.frustration_model is None:
logger.error("Models not loaded. Call train_models() or load_models() first.")
return None
            
# Extract features from dictionary
feature_vector = []
for key in sorted(features.keys()):
if isinstance(features[key], (int, float)):
feature_vector.append(features[key])
        
# Reshape to 2D array (required for scikit-learn)
X = np.array(feature_vector).reshape(1, -1)
        
# Standardize features
X_scaled = self.scaler.transform(X)
        
# Make predictions
fatigue_prob = self.fatigue_model.predict_proba(X_scaled)[0, 1]
frustration_prob = self.frustration_model.predict_proba(X_scaled)[0, 1]
        
# Calculate neutral probability (1 minus the others, ensuring it's not negative)
neutral_prob = max(0, 1.0 - (fatigue_prob + frustration_prob))
        
# Determine cognitive state based on thresholds and confidence
cognitive_state = 'neutral'
fatigue_detected = fatigue_prob >= fatigue_threshold
frustration_detected = frustration_prob >= frustration_threshold
        
if fatigue_detected and frustration_detected:
# Both states detected, choose based on higher confidence
if prefer_higher_confidence:
cognitive_state = 'fatigue' if fatigue_prob > frustration_prob else 'frustration'
else:
# Default to frustration as it might require more immediate attention
cognitive_state = 'frustration'
elif fatigue_detected:
cognitive_state = 'fatigue'
elif frustration_detected:
cognitive_state = 'frustration'
        else:
            # Default to frustration as it might require more immediate attention
            cognitive_state = 'frustration'
    elif fatigue_detected:
        cognitive_state = 'fatigue'
    elif frustration_detected:
        cognitive_state = 'frustration'
        
    # Return complete prediction results
    return {
        'cognitive_state': cognitive_state,
        'fatigue_probability': fatigue_prob,
        'frustration_probability': frustration_prob,
        'neutral_probability': neutral_prob,
        'confidence': max(fatigue_prob, frustration_prob, neutral_prob),
        'thresholds': {
            'fatigue': fatigue_threshold,
            'frustration': frustration_threshold
        }
    }
    
def evaluate_models(self, features_val):
    """
    Evaluate trained models using validation data.
        
        Parameters:
        -----------
        features_val : dict
            Dictionary containing validation features
            
        Returns:
        --------
        dict
            Dictionary containing evaluation metrics
        """
        logger.info("Evaluating cognitive state detection models")
        
        if self.fatigue_model is None or self.frustration_model is None:
            logger.error("Models not loaded. Call train_models() or load_models() first.")
            return None
            
        # Prepare validation feature vectors and labels
        X_val, y_fatigue_val, y_frustration_val = self._prepare_feature_vectors(features_val)
        
        # Standardize features
        X_val_scaled = self.scaler.transform(X_val)
        
        # Evaluate fatigue model
        logger.info("Evaluating fatigue model")
        fatigue_preds = self.fatigue_model.predict(X_val_scaled)
        fatigue_probs = self.fatigue_model.predict_proba(X_val_scaled)[:, 1]
        
        # Calculate metrics
        fatigue_accuracy = accuracy_score(y_fatigue_val, fatigue_preds)
        fatigue_precision = precision_score(y_fatigue_val, fatigue_preds)
        fatigue_recall = recall_score(y_fatigue_val, fatigue_preds)
        fatigue_f1 = f1_score(y_fatigue_val, fatigue_preds)
        fatigue_auc = roc_auc_score(y_fatigue_val, fatigue_probs)
        fatigue_cm = confusion_matrix(y_fatigue_val, fatigue_preds)
        
        # Evaluate frustration model
        logger.info("Evaluating frustration model")
        frustration_preds = self.frustration_model.predict(X_val_scaled)
        frustration_probs = self.frustration_model.predict_proba(X_val_scaled)[:, 1]
        
        # Calculate metrics
        frustration_accuracy = accuracy_score(y_frustration_val, frustration_preds)
        frustration_precision = precision_score(y_frustration_val, frustration_preds)
        frustration_recall = recall_score(y_frustration_val, frustration_preds)
        frustration_f1 = f1_score(y_frustration_val, frustration_preds)
        frustration_auc = roc_auc_score(y_frustration_val, frustration_probs)
        frustration_cm = confusion_matrix(y_frustration_val, frustration_preds)
        
        # Create results dictionary
        results = {
            'fatigue': {
                'accuracy': fatigue_accuracy,
                'precision': fatigue_precision,
                'recall': fatigue_recall,
                'f1': fatigue_f1,
                'auc': fatigue_auc,
                'confusion_matrix': fatigue_cm
            },
            'frustration': {
                'accuracy': frustration_accuracy,
                'precision': frustration_precision,
                'recall': frustration_recall,
                'f1': frustration_f1,
                'auc': frustration_auc,
                'confusion_matrix': frustration_cm
            }
        }
        
        logger.info(f"Fatigue model: Accuracy={fatigue_accuracy:.4f}, F1={fatigue_f1:.4f}, AUC={fatigue_auc:.4f}")
        logger.info(f"Frustration model: Accuracy={frustration_accuracy:.4f}, F1={frustration_f1:.4f}, AUC={frustration_auc:.4f}")
        
        # Get feature importance for Random Forest (fatigue model)
        if hasattr(self.fatigue_model, 'feature_importances_'):
            fatigue_importances = self.fatigue_model.feature_importances_
        else:
            fatigue_importances = None
        
        # Get feature importance for SVM (frustration model)
        if hasattr(self.frustration_model, 'coef_'):
            frustration_importances = self.frustration_model.coef_[0]
        else:
            frustration_importances = None
        
        return results, fatigue_importances, frustration_importances
    
    def perform_ablation_studies(self, features, features_val):
        """
        Perform ablation studies to analyze feature contribution.
        
        Parameters:
        -----------
        features : dict
            Dictionary containing training features
        features_val : dict
            Dictionary containing validation features
            
        Returns:
        --------
        dict
            Dictionary containing ablation study results
        """
        logger.info("Performing ablation studies")
        
        # Define feature groups
        feature_groups = [
            'blink_patterns',
            'eye_closure_patterns',
            'pupil_metrics',
            'gaze_patterns'
        ]
        
        results = {}
        
        # Train with all features (baseline)
        logger.info("Training baseline models with all features")
        baseline_models = self.train_models(features, grid_search=False)
        baseline_results, _, _ = self.evaluate_models(features_val)
        results['all_features'] = baseline_results
        
        # Train removing one feature group at a time
        for group in feature_groups:
            logger.info(f"Training models without {group}")
            
            # Create a copy of features with the specified group removed
            reduced_features = features.copy()
            reduced_features[group] = []
            
            # Train models
            self.train_models(reduced_features, grid_search=False)
            
            # Evaluate
            reduced_results, _, _ = self.evaluate_models(features_val)
            results[f'without_{group}'] = reduced_results
        
        logger.info("Ablation studies complete")
        
        return results


def prepare_feature_importance_analysis(fatigue_importances, frustration_importances, feature_names=None):
    """
    Prepare feature importance analysis for visualization.
    
    Parameters:
    -----------
    fatigue_importances : array-like
        Feature importances from fatigue model
    frustration_importances : array-like
        Feature importances from frustration model
    feature_names : list, optional
        Names of features
        
    Returns:
    --------
    dict
        Dictionary containing feature importance data
    """
    # If feature names not provided, use generic names
    if feature_names is None:
        feature_names = [f'Feature {i+1}' for i in range(len(fatigue_importances))]
        
    # Ensure consistent length
    n_features = min(len(fatigue_importances) if fatigue_importances is not None else 0,
                    len(frustration_importances) if frustration_importances is not None else 0,
                    len(feature_names))
                    
    feature_names = feature_names[:n_features]
    
    # Prepare importance data
    if fatigue_importances is not None:
        fatigue_data = fatigue_importances[:n_features]
    else:
        fatigue_data = np.zeros(n_features)
        
    if frustration_importances is not None:
        frustration_data = frustration_importances[:n_features]
        
        # Convert negative values to absolute values for SVM coefficients
        if np.any(frustration_data < 0):
            frustration_data = np.abs(frustration_data)
    else:
        frustration_data = np.zeros(n_features)
    
    # Sort features by combined importance
    combined_importance = fatigue_data + frustration_data
    sorted_indices = np.argsort(combined_importance)[::-1]
    
    # Select top features (maximum 10 for clarity)
    top_indices = sorted_indices[:10]
    
    # Create result dictionary
    result = {
        'features': [feature_names[i] for i in top_indices],
        'fatigue': [float(fatigue_data[i]) for i in top_indices],
        'frustration': [float(frustration_data[i]) for i in top_indices]
    }
    
    return result
